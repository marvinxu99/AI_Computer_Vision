{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Understanding with Captum\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Captum’s approach to model interpretability is in terms of attributions. There are three kinds of attributions available in Captum:\n",
    "\n",
    "Feature Attribution seeks to explain a particular output in terms of features of the input that generated it. Explaining whether a movie review was positive or negative in terms of certain words in the review is an example of feature attribution.\n",
    "\n",
    "Layer Attribution examines the activity of a model’s hidden layer subsequent to a particular input. Examining the spatially-mapped output of a convolutional layer in response to an input image in an example of layer attribution.\n",
    "\n",
    "Neuron Attribution is analagous to layer attribution, but focuses on the activity of a single neuron.\n",
    "\n",
    "In this interactive notebook, we’ll look at Feature Attribution and Layer Attribution.\n",
    "\n",
    "Each of the three attribution types has multiple attribution algorithms associated with it. Many attribution algorithms fall into two broad categories:\n",
    "\n",
    "Gradient-based algorithms calculate the backward gradients of a model output, layer output, or neuron activation with respect to the input. Integrated Gradients (for features), Layer Gradient * Activation, and Neuron Conductance are all gradient-based algorithms.\n",
    "\n",
    "Perturbation-based algorithms examine the changes in the output of a model, layer, or neuron in response to changes in the input. The input perturbations may be directed or random. Occlusion, Feature Ablation, and Feature Permutation are all perturbation-based algorithms.\n",
    "\n",
    "We’ll be examining algorithms of both types below.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
